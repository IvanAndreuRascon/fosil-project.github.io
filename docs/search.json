[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Info here"
  },
  {
    "objectID": "posts/literate-programming/index.html",
    "href": "posts/literate-programming/index.html",
    "title": "Literate programming",
    "section": "",
    "text": "Literate programming refers to the integration of code and prose in a reproducible document. This practice is not yet mainstream in linguistics, although it holds several advantages as opposed to traditional reporting methods. Traditionally, statistical analysis, plots, tables, citations and captions would be created and manually and inserted into a manuscript. One potential issue with this approach is the increased probability of reported errors. For example, a recent study found that…(Roettger analysis of Labphon). A literate programming approach to manuscript creation would plausibly reduce the quantity of these errors, and it would make the correct information traceable more often. Additionally, updates to the data would be (almost) automatically integrated into a given manuscript if the necssary scripts are run again.\nThe present tutorial will provide an example of literate programming specifically for linguists by using an open dataset in linguistics and reported a mock analysis While the emphasis of this tutorial will be on creating a simple working example in Rmarkdown, it is important to note that literate programming can be applied within R to APA style manuscripts (see the Papaja package), in slideshows (see Xaringan) and in other programs entirely (qmd, python, jupitor notebooks)"
  },
  {
    "objectID": "posts/literate-programming/index.html#reporting-descriptive-statistics",
    "href": "posts/literate-programming/index.html#reporting-descriptive-statistics",
    "title": "Literate programming",
    "section": "Reporting descriptive statistics",
    "text": "Reporting descriptive statistics\nIn general all, inline reporting occurs in Rmardown between backticks, i.e., ` `. Specifically, you have to wrap the r code with `r ` to integrate it into your document. For instance, if we want to report the overall mean for the column DurationOfPrefix, we can simply put r code such as, mean(durationsGe$DurationOfPrefix) between to back ticks like this:\n\n\nThe mean duration is `r mean(durationsGe$DurationOfPrefix)`. \n\n\n\nWhich would be rendered as:\n\nThe mean duration is 0.1252515\n\nThere are several decimal points here, though! We probably don’t want that, so if we haven’t rounded the data previously, we can do so inline by using the round function:\n\n\nThe mean duration was `r round(mean(durationsGe$DurationOfPrefix), digits = 2)`. \n\n\n\nNow the code is rendered in prose as:\n\nThe mean duration was 0.13.\n\nAs you can see, this can get rather long in a hurry. Another option is to use an code chunk to calculate summary statistics and assign them to objects. Then you can simply use the objects with inline chunks. For instance, we likely also want to report how many participants are in our dataset. Let’s do this and report it in prose.\n\nCodemean_duration  <- round(mean(durationsGe$DurationOfPrefix), digits = 2)\nn_participants <- length(unique(durationsGe$Speaker))\n\n\n\n\nThere were `r n_participants` participants. \nThe mean duration was `r mean_duration`. \n\n\n\n\nThere were 132 participants. The mean duration was 0.13."
  },
  {
    "objectID": "posts/literate-programming/index.html#reporting-results-of-statistical-models",
    "href": "posts/literate-programming/index.html#reporting-results-of-statistical-models",
    "title": "Literate programming",
    "section": "Reporting results of statistical models",
    "text": "Reporting results of statistical models\nWe can also report the output statistical models and tests. Typically, the results of these tests can be stored in an object in R and extracted. I will provide an example with a t-test in R. First, we will run a t-test to see whether duration varies as a function of speaker sex:\n\nCodet_test_object <- t.test(DurationOfPrefix ~ Sex, data = durationsGe)\nt_test_object\n\n\n    Welch Two Sample t-test\n\ndata:  DurationOfPrefix by Sex\nt = -0.1949, df = 413.26, p-value = 0.8456\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -0.009955279  0.008159221\nsample estimates:\nmean in group female   mean in group male \n           0.1249141            0.1258121 \n\n\nFor a t-test, in APA guidelines we report degrees of freedom, the t-value, and the p-value. All of these are actually stored in the object we just created, and we can automate the reporting process.\nNote: The degree of freedom in this dataset are exaggerated due to the nested structure of the data and this t-test serves as an example only\nDegrees of Freedom\n\nr round(t_test_object$parameter, digits = 2)\n\n\n413.26\n\nThe t-value\n\nr round(t_test_object$statistic, digits = 2)\n\n\n-0.19\n\nThe p-value\n\nr round(t_test_object$p.value, digits = 2)\n\n\n0.85\n\nAll together\n\nt(r round(t_test_object\\(parameter, digits = 2)) = r round(t_test_object\\)statistic, digits = 2), p = r round(t_test_object$p.value, digits = 2)\n\n\nt(413.26) = -0.19, p = 0.85"
  },
  {
    "objectID": "posts/open-data/index.html",
    "href": "posts/open-data/index.html",
    "title": "Open data",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/positionality-statements/index.html",
    "href": "posts/positionality-statements/index.html",
    "title": "Positionality statements",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/preprints/index.html",
    "href": "posts/preprints/index.html",
    "title": "Preprints",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/preregistration/index.html",
    "href": "posts/preregistration/index.html",
    "title": "Preregistration",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/registered-reports/index.html",
    "href": "posts/registered-reports/index.html",
    "title": "Registered reports",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/reproducible-code-projects/index.html",
    "href": "posts/reproducible-code-projects/index.html",
    "title": "Reproducible code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/what-is-open-science/index.html",
    "href": "posts/what-is-open-science/index.html",
    "title": "What is open science?",
    "section": "",
    "text": "What is open science? Parsons et al. (2022) provide the following definition:\n\nAn umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.\n\nThat sounds wonderful, right? But you might be asking yourself why the push for Open Science? It may come as a surprise to some, but the open, transparent research practices described by Parsons et al. (2022) have not been the norm in scholarly research.\nTo properly contextualize the need for Open Science, we have to go back to the early 2010’s. Around this time, several fields of research embarked on large-scale replication projects to scrutinize some of their major findings. One example of these projects took place in psychology. This particular project tested whether they could replicate 100 influential findings (Open Science Collaboration 2015). They found the approximately 53% of the findings did not replicate. This project inspired similar large-scale replication projects in other fields, yielding similar results in economics (Camerer et al. 2016), social sciences (Camerer et al. 2018), and cancer research (Errington et al. 2021). These alarming findings are now referred to as the replication (or reproducibility) crisis. Researchers have pointed to questionable research practices (QRPs), p-hacking, HARKing, small sample sizes, poor theory, lack of transparency, etc. as factors that ultimately led to the replication crisis, though it is likely that other factors are at play.\n\nTake a second to consider your field of study. How many important findings do you think would replicate? If you were to replicate 100 of the most influential findings, how many would need to replicate for you to have confidence in your field?\n\nIn the aftermath of the replication crisis we have seen a push for increased transparency and reproducible methodology to help mitigate the effects of questionable research practices. The resulting methodological framework and associated techniques have reshaped research methods in psychology and have slowly but surely made their way into adjacent fields. This website is dedicated to making open science practices understandable and accessible to researchers in the speech sciences from all backgrounds and at every stage, from students/early career researchers to senior researchers.\nTo this end, we have highlighted 7 areas in which speech researchers can engage in Open Science:\n\nLiterate programming\nOpen data\nPositionality statements\nPreprints\nPreregistration\nRegistered reports\nReproducible code/projects\n\nThroughout this website you will find tutorials designed to get you up and running in each of these areas so that you can engage in Open Science practices.\nSee Figure 1\n\n\n\n\n\nFigure 1: This is a caption\n\n\n\n\n\n\n\n\nReferences\n\nCamerer, Colin F, Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2016. “Evaluating Replicability of Laboratory Experiments in Economics.” Science 351 (6280): 1433–36. https://doi.org/10.1126/science.aaf091.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015.” Nature Human Behaviour 2 (9): 637–44. https://doi.org/10.1038/s41562-018-0399-z.\n\n\nErrington, Timothy M, Maya Mathur, Courtney K Soderberg, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021. “Investigating the Replicability of Preclinical Cancer Biology.” Elife 10: e71601. https://doi.org/10.7554/eLife.71601.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nParsons, Sam, Flávio Azevedo, Mahmoud M Elsherif, Samuel Guay, Owen N Shahim, Gisela H Govaart, Emma Norris, et al. 2022. “A Community-Sourced Glossary of Open Scholarship Terms.” Nature Human Behaviour 6 (3): 312–18. https://doi.org/10.1038/s41562-021-01269-4.\n\nCitationBibTeX citation:@online{v.casillas2023,\n  author = {Joseph V. Casillas},\n  title = {What Is Open Science?},\n  date = {2023-02-21},\n  url = {https://FOSIL-project.github.io/what-is-open-science/index.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJoseph V. Casillas. 2023. “What Is Open Science?” February\n21, 2023. https://FOSIL-project.github.io/what-is-open-science/index.html."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nWhat is open science?\n\n\n\n\n\n\n\ninfo\n\n\n\n\nA quick primer on Open Science and reproducible research.\n\n\n\n\n\n\nFeb 21, 2023\n\n\nJoseph V. Casillas\n\n\n\n\n\n\n  \n\n\n\n\nLiterate programming\n\n\n\n\n\n\n\ninfo\n\n\ncoding\n\n\nliterate programming\n\n\n\n\nIntroduction to literate programming.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nKyle Parish, Isabelle Chang\n\n\n\n\n\n\n  \n\n\n\n\nOpen data\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nTBD\n\n\n\n\n\n\n  \n\n\n\n\nPositionality statements\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nPreprints\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nTBD\n\n\n\n\n\n\n  \n\n\n\n\nPreregistration\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nTBD\n\n\n\n\n\n\n  \n\n\n\n\nRegistered reports\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nTBD\n\n\n\n\n\n\n  \n\n\n\n\nReproducible code/projects\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nTBD\n\n\n\n\n\n\nNo matching items"
  }
]